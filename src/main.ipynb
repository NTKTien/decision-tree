{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19a59fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aaa26",
   "metadata": {},
   "source": [
    "Hàm tạo dataset của chị Tuyết \n",
    "\n",
    "\"Tại đây\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbcb13f",
   "metadata": {},
   "source": [
    "Bài của Thiện"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9705b42b",
   "metadata": {},
   "source": [
    "1. Huấn luyện với từng tỷ lệ chia: 40/60, 60/40, 80/20, 90/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d937ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "\n",
    "def train_and_save_decision_trees(data_splits, proportions, seed=42):\n",
    "    \"\"\"\n",
    "    Train decision tree models for various data splits and save them to disk.\n",
    "\n",
    "    Parameters:\n",
    "        data_splits (list): List of tuples with (X_train, X_test, y_train, y_test).\n",
    "        proportions (list): List of tuples with (train_ratio, test_ratio).\n",
    "        seed (int, optional): Seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        list: Collection of trained decision tree models.\n",
    "    \"\"\"\n",
    "    # Create directory for saving models if it doesn't exist\n",
    "    save_dir = \"decision_trees\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize list to store trained models\n",
    "    trained_trees = []\n",
    "\n",
    "    # Process each data split and proportion\n",
    "    for idx, (data, ratio) in enumerate(zip(data_splits, proportions)):\n",
    "        train_features, _, train_labels, _ = data\n",
    "        train_ratio, test_ratio = ratio\n",
    "\n",
    "        # Configure and train decision tree\n",
    "        tree = DecisionTreeClassifier(\n",
    "            criterion=\"entropy\",\n",
    "            splitter=\"best\",\n",
    "            random_state=seed\n",
    "        )\n",
    "        tree.fit(train_features, train_labels)\n",
    "\n",
    "        # Store trained model\n",
    "        trained_trees.append(tree)\n",
    "\n",
    "        # Generate filename based on split ratios\n",
    "        file_path = os.path.join(save_dir, f\"dt_model_{int(train_ratio*100)}_{int(test_ratio*100)}.pkl\")\n",
    "\n",
    "        # Save model to disk\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tree, f)\n",
    "\n",
    "        # Print confirmation\n",
    "        print(f\"Model for {int(train_ratio*100)}:{int(test_ratio*100)} split trained and saved to {file_path}\")\n",
    "\n",
    "    return trained_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a377600",
   "metadata": {},
   "source": [
    "2. Hình dung cây quyết định với các tỷ lệ phân chia khác nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae10f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "def plot_decision_trees(models, depths, feature_labels, class_labels):\n",
    "    \"\"\"\n",
    "    Generate visualizations for decision tree models and save them as PNG files.\n",
    "\n",
    "    Parameters:\n",
    "        models (list): List of trained DecisionTreeClassifier instances.\n",
    "        depths (list): List of integers specifying max depths for each model.\n",
    "        feature_labels (list): Names of features used in the models.\n",
    "        class_labels (list): Names of target classes in the dataset.\n",
    "    \"\"\"\n",
    "    # Define output directory\n",
    "    output_dir = \"trees_visualization\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each model and depth\n",
    "    for idx, (tree_model, depth) in enumerate(zip(models, depths)):\n",
    "        # Generate dot format for the tree\n",
    "        tree_dot = export_graphviz(\n",
    "            tree_model,\n",
    "            feature_names=feature_labels,\n",
    "            class_names=class_labels,\n",
    "            filled=True,\n",
    "            rounded=True,\n",
    "            special_characters=True\n",
    "        )\n",
    "\n",
    "        # Create Graphviz object\n",
    "        tree_graph = graphviz.Source(tree_dot, format=\"png\")\n",
    "        tree_graph.engine = \"dot\"\n",
    "\n",
    "        # Define file path for saving\n",
    "        file_name = f\"{output_dir}/tree_split_80_20_depth_{depth}.png\"\n",
    "        \n",
    "        # Save the visualization\n",
    "        tree_graph.render(filename=file_name, cleanup=True)\n",
    "\n",
    "        # Display in interactive environment\n",
    "        display(tree_graph)\n",
    "\n",
    "        # Print status message\n",
    "        print(f\"Tree with depth {depth} for 80:20 split visualized and saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfe9681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def assess_decision_trees(models, data_splits, ratios):\n",
    "    \"\"\"\n",
    "    Assess trained decision tree models by generating classification reports and confusion matrix heatmaps.\n",
    "\n",
    "    Parameters:\n",
    "        models (list): List of trained decision tree models.\n",
    "        data_splits (list): List of tuples containing (X_train, X_test, y_train, y_test).\n",
    "        ratios (list): List of tuples with (train_ratio, test_ratio).\n",
    "\n",
    "    Returns:\n",
    "        None: Saves classification reports and confusion matrix heatmaps as PNG files.\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_dir = \"evaluation_reports\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each model, dataset, and ratio\n",
    "    for idx, (model, data, (train_ratio, test_ratio)) in enumerate(zip(models, data_splits, ratios)):\n",
    "        _, test_features, _, test_labels = data\n",
    "\n",
    "        # Generate predictions\n",
    "        predictions = model.predict(test_features)\n",
    "\n",
    "        # Compute classification report as a dictionary\n",
    "        report_dict = classification_report(test_labels, predictions, zero_division=1, digits=4, output_dict=True)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "        # Create figure with two subplots\n",
    "        fig = plt.figure(figsize=(14, 7))\n",
    "\n",
    "        # Subplot 1: Classification report as a single table\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        table_data = []\n",
    "        table_rows = []\n",
    "\n",
    "        # Extract class metrics\n",
    "        for label in report_dict:\n",
    "            if label not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "                table_data.append([\n",
    "                    report_dict[label]['precision'],\n",
    "                    report_dict[label]['recall'],\n",
    "                    report_dict[label]['f1-score'],\n",
    "                    report_dict[label]['support']\n",
    "                ])\n",
    "                table_rows.append(label)\n",
    "        \n",
    "        # Add accuracy, macro avg, weighted avg\n",
    "        table_data.append([None, None, report_dict['accuracy'], None])\n",
    "        table_rows.append('accuracy')\n",
    "        for avg in ['macro avg', 'weighted avg']:\n",
    "            table_data.append([\n",
    "                report_dict[avg]['precision'],\n",
    "                report_dict[avg]['recall'],\n",
    "                report_dict[avg]['f1-score'],\n",
    "                report_dict[avg]['support']\n",
    "            ])\n",
    "            table_rows.append(avg)\n",
    "\n",
    "        # Create table\n",
    "        table = ax1.table(\n",
    "            cellText=[[f\"{x:.4f}\" if isinstance(x, float) else x or \"\" for x in row] for row in table_data],\n",
    "            colLabels=['Precision', 'Recall', 'F1-Score', 'Support'],\n",
    "            rowLabels=table_rows,\n",
    "            loc='center',\n",
    "            cellLoc='center',\n",
    "            colWidths=[0.2, 0.2, 0.2, 0.2],\n",
    "            bbox=[0.1, 0.1, 0.8, 0.8]\n",
    "        )\n",
    "        table.set_fontsize(10)\n",
    "        for (i, j), cell in table.get_celld().items():\n",
    "            cell.set_height(0.08)\n",
    "            cell.set_text_props(ha='center', va='center')\n",
    "        ax1.axis('off')\n",
    "        ax1.set_title('Classification Metrics')\n",
    "\n",
    "        # Subplot 2: Confusion matrix heatmap\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='viridis',\n",
    "            cbar=True,\n",
    "            xticklabels=model.classes_,\n",
    "            yticklabels=model.classes_,\n",
    "            square=True\n",
    "        )\n",
    "        ax2.set_xlabel('Predicted')\n",
    "        ax2.set_ylabel('Actual')\n",
    "        ax2.set_title('Confusion Matrix')\n",
    "\n",
    "        # Set figure title\n",
    "        fig.suptitle(f'Evaluation for {int(train_ratio*100)}:{int(test_ratio*100)} Split', fontsize=14)\n",
    "\n",
    "        # Save figure\n",
    "        file_path = os.path.join(output_dir, f'eval_report_{int(train_ratio*100)}_{int(test_ratio*100)}.png')\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(file_path)\n",
    "        plt.show()\n",
    "\n",
    "        # Print confirmation\n",
    "        print(f\"Evaluation for {int(train_ratio*100)}:{int(test_ratio*100)} split saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38f23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68293807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_depth_varied_trees(data_split, depth_values):\n",
    "    \"\"\"\n",
    "    Train decision tree models with varying max depths and compute their test accuracies.\n",
    "\n",
    "    Parameters:\n",
    "        data_split (tuple): Tuple containing (X_train, X_test, y_train, y_test).\n",
    "        depth_values (list): List of max depth values (integers or None) for training models.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list of trained models, dictionary mapping max depths to test accuracies)\n",
    "    \"\"\"\n",
    "    # Create directory for saving models\n",
    "    save_dir = \"decision_trees/depth_models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize lists and dictionaries\n",
    "    trained_trees = []\n",
    "    accuracy_map = {}\n",
    "\n",
    "    # Extract data\n",
    "    X_train, X_test, y_train, y_test = data_split\n",
    "\n",
    "    # Train models for each max depth\n",
    "    for depth in depth_values:\n",
    "        # Configure decision tree\n",
    "        tree = DecisionTreeClassifier(\n",
    "            criterion=\"entropy\",\n",
    "            max_depth=depth,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        tree.fit(X_train, y_train)\n",
    "\n",
    "        # Store model\n",
    "        trained_trees.append(tree)\n",
    "\n",
    "        # Compute accuracy\n",
    "        predictions = tree.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        accuracy_map[depth] = accuracy\n",
    "\n",
    "        # Save model\n",
    "        depth_str = str(depth) if depth is not None else \"none\"\n",
    "        file_path = os.path.join(save_dir, f\"tree_depth_{depth_str}.pkl\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(tree, f)\n",
    "\n",
    "        # Print confirmation\n",
    "        print(f\"Trained and saved model with max_depth={depth_str} to {file_path}\")\n",
    "\n",
    "    return trained_trees, accuracy_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
